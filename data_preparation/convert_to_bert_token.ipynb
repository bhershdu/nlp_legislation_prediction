{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In order to train on the words in a legislative bill we want to make use of an existing Natural Language Processing (NLP) model in order to come up with a data structure that will encode contextual information.\n",
    "\n",
    "To this end we use the BERT model, which is one of the modern NLP models that takes into account context.\n",
    "\n",
    "For this project, we want to take the text from the bill and predict which party sponsored it"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First steps\n",
    "\n",
    "Load the BERT model using pytorch.\n",
    "\n",
    "Why pytorch, this is higher level libary for constructing machine learning models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the we have the model, we want to run inference on the text and get the output token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def generate_token(summary_json):\n",
    "    \"\"\"\n",
    "    execute the BERT model on the title section of the summary\n",
    "    :param summary_json: the data from a summary file\n",
    "    :return: a numpy vector\n",
    "    \"\"\"\n",
    "    text = summary_json['text']\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    print(\"encoded input\")\n",
    "    print(encoded_input)\n",
    "    print(\"model output\")\n",
    "    output = model(**encoded_input)\n",
    "    last_shape = output.last_hidden_state.shape\n",
    "    elements = np.cumproduct(last_shape)\n",
    "    last_layer_vector = output.last_hidden_state.detach().numpy().reshape(max(elements))\n",
    "    return last_layer_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading summary_bill_1811_1393180.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101, 11336, 26304,  1106,  1103,  5600,  1104,  1671, 11709,  3641,\n",
      "          1105,  1103,  1671, 10614,  3641,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1811_1393180.npy\n",
      "reading summary_bill_1811_1393181.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101, 11336, 26304,  1106, 13178,  4237,  1107,  2078, 10713,  1411,\n",
      "           117,  1278,  1629,   117,  1137,  1491,  1629,  5845,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1811_1393181.npy\n",
      "reading summary_bill_1811_1470063.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101, 10697, 12647, 24594,  1111,  1103, 11928,  1104,  2218,  7844,\n",
      "          1104,  1103,  1352,  1111, 12087,  1201,  3830,  1340,  1476,   117,\n",
      "         17881,  1477,  1105,  1340,  1476,   117, 17881,  1495,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1811_1470063.npy\n",
      "reading summary_bill_1811_1506887.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101, 11336,  9823,  8702, 21290,  7157,  1206,  1203,  6446,  1105,\n",
      "          6036,   117, 11653,  1103, 11778,  1880,  1104, 20557,  2597,  4125,\n",
      "           117,  1105,  4374,  6036,  1107,  1103,  1835,  1661,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1811_1506887.npy\n",
      "reading summary_bill_1959_1542899.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101, 11336, 26304,  1106,  1103, 15156,  1137, 20776,  1104,  1858,\n",
      "          2393, 12013,  3382,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1959_1542899.npy\n",
      "reading summary_bill_1959_1545862.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101, 11336, 26304,  1106, 11631,  5783,  1104, 19696,  1106,  1103,\n",
      "          1352,  1459,  3463,  1105,  1352,  1783,  3463,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1959_1545862.npy\n",
      "reading summary_bill_1959_1546074.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101, 11336, 26304,  1106,  1473,  6245,  1104,  1148,  6297,  1468,\n",
      "          1150,  2939,  1121,  5680,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1959_1546074.npy\n",
      "reading summary_bill_1959_1546096.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101,   138,  8661, 12148,  1988,  1158,  1352,  4702,  4210,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1959_1546096.npy\n",
      "reading summary_bill_1959_1546341.json\n",
      "encoded input\n",
      "{'input_ids': tensor([[  101, 11336, 26304,  1106,  1103,  5600,  1104,  1671, 11709,  3641,\n",
      "          1105,  1103,  1671, 10614,  3641,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "saving C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\data\\tokenized\\summary_bill_1959_1546341.npy\n"
     ]
    }
   ],
   "source": [
    "parent_path = os.path.dirname(os.getcwd())\n",
    "search_path = os.path.join(parent_path, \"data\", \"extracted\")\n",
    "token_path = os.path.join(parent_path, \"data\", \"tokenized\")\n",
    "\n",
    "for root, dirs, files in os.walk(search_path):\n",
    "    for f in files:\n",
    "        with open(os.path.join(root, f), 'r') as s_file:\n",
    "            print(f'reading {f}')\n",
    "            summary = json.load(s_file)\n",
    "            encoding = generate_token(summary)\n",
    "            output_path = os.path.join(token_path, f.replace(\".json\", \".npy\"))\n",
    "            print(f'saving {output_path}')\n",
    "            np.save(output_path, encoding,allow_pickle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# how dissimilar are the encodings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "source       object\nencoding    float32\ndtype: object"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets reread in the vectors\n",
    "encoded_data = {}\n",
    "for root, dirs, files in os.walk(token_path):\n",
    "    for f in files:\n",
    "        encoding = np.load(os.path.join(root, f))\n",
    "        encoded_data['source'] = f\n",
    "        encoded_data['encoding'] = encoding\n",
    "encoding_df = pd.DataFrame(encoded_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}