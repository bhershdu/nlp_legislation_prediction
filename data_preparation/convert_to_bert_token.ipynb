{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In order to train on the words in a legislative bill we want to make use of an existing Natural Language Processing (NLP) model in order to come up with a data structure that will encode contextual information.\n",
    "\n",
    "To this end we use the BERT model, which is one of the modern NLP models that takes into account context.\n",
    "\n",
    "For this project, we want to take the text from the bill and predict which party sponsored it"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First steps\n",
    "\n",
    "Load the BERT model using pytorch.\n",
    "\n",
    "Why pytorch, this is higher level libary for constructing machine learning models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the we have the model, we want to run inference on the text and get the output token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_token(summary_json):\n",
    "    \"\"\"\n",
    "    execute the BERT model on the title section of the summary\n",
    "    :param summary_json: the data from a summary file\n",
    "    :return: a numpy vector\n",
    "    \"\"\"\n",
    "    text = summary_json['text']\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    print(\"encoded input\")\n",
    "    print(encoded_input)\n",
    "    print(\"model output\")\n",
    "    output = model(**encoded_input)\n",
    "#    print(type(output))\n",
    "    last_shape = output.last_hidden_state.shape\n",
    "    elements = np.cumproduct(last_shape)\n",
    "    last_layer_vector = torch.from_numpy(output.last_hidden_state.detach().numpy().reshape(max(elements)))\n",
    "#    print(type(last_layer_vector))\n",
    "    return last_layer_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parent_path = os.path.dirname(os.getcwd())\n",
    "search_path = os.path.join(parent_path, \"data\", \"extracted\")\n",
    "token_path = os.path.join(parent_path, \"data\", \"tokenized\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "for root, dirs, files in os.walk(search_path):\n",
    "    for f in files:\n",
    "        with open(os.path.join(root, f), 'r') as s_file:\n",
    "            print(f'reading {f}')\n",
    "            summary = json.load(s_file)\n",
    "            encoding = generate_token(summary)\n",
    "            print(f'{len(encoding)} of type {type(encoding)}')\n",
    "            summary[\"input_full\"] = encoding\n",
    "            df = pd.DataFrame(summary)\n",
    "            output_path = os.path.join(token_path, f.replace(\".json\",\".pkl\"))\n",
    "            print(f'saving {output_path}')\n",
    "            df.to_pickle(output_path,compression=\"gzip\",protocol=pickle.DEFAULT_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These tensors all have different sizes, so running a cosine similarity will not work.\n",
    "I will try to compress these down to a standard size.\n",
    "Let's try this  : https://newbedev.com/downsample-a-1d-numpy-array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's reload and the dfs and save a resampled version of the encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def ResampleLinear1D(original, targetLen):\n",
    "    original = np.array(original, dtype=np.float)\n",
    "\n",
    "    index_arr = np.linspace(0, len(original)-1, num=targetLen, dtype=np.float)\n",
    "    index_floor = np.array(index_arr, dtype=np.int) #Round down\n",
    "\n",
    "    index_ceil = index_floor + 1\n",
    "    index_rem = index_arr - index_floor #Remain\n",
    "\n",
    "    val1 = original[index_floor]\n",
    "    val2 = original[index_ceil % len(original)]\n",
    "    interp = val1 * (1.0-index_rem) + val2 * index_rem\n",
    "    assert(len(interp) == targetLen)\n",
    "    return interp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for root,dirs, files in os.walk(token_path):\n",
    "    for f in files:\n",
    "        pkl_file_path = os.path.join(root, f)\n",
    "        df = pd.read_pickle(pkl_file_path, compression=\"gzip\")\n",
    "        print(df)\n",
    "        shrunk = ResampleLinear1D(df[\"input_full\"], 2048)\n",
    "        print(shrunk)\n",
    "        # \"status\": 2, \"party\": 1\n",
    "        df2 = pd.DataFrame({\"status\": df[\"status\"][0:2048],\n",
    "                            \"party\": df[\"party\"][0:2048],\n",
    "                            \"input_shrunk\": shrunk})\n",
    "        print(df2.shape)\n",
    "        df2.to_pickle(pkl_file_path.replace(\".pkl\",\"-shrunk.pkl\"),compression=\"gzip\", protocol=pickle.DEFAULT_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}