{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TitlePartyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TitlePartyModel, self).__init__()\n",
    "        self.input = torch.nn.Linear(2048,2048, dtype=torch.float32)\n",
    "        self.input_activation = torch.nn.Sigmoid()\n",
    "        self.hidden1 = torch.nn.Linear(2048,1024)\n",
    "        self.hidden1_activation = torch.nn.Sigmoid()\n",
    "        self.hidden2 = torch.nn.Linear(1024,128)\n",
    "        self.hidden2_activation = torch.nn.Sigmoid()\n",
    "        # 4 political party choices\n",
    "        self.hidden3 = torch.nn.Linear(128,4)\n",
    "        self.output = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.input_activation(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden1_activation(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.hidden2_activation(x)\n",
    "        x = self.hidden3(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model\n",
      "TitlePartyModel(\n",
      "  (input): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (input_activation): Sigmoid()\n",
      "  (hidden1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (hidden1_activation): Sigmoid()\n",
      "  (hidden2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (hidden2_activation): Sigmoid()\n",
      "  (hidden3): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (output): Softmax(dim=None)\n",
      ")\n",
      "just the 2nd layer\n",
      "Linear(in_features=2048, out_features=1024, bias=True)\n",
      "parameters\n",
      "Parameter containing:\n",
      "tensor([[-0.0114,  0.0177,  0.0192,  ..., -0.0149,  0.0153,  0.0162],\n",
      "        [-0.0155,  0.0038, -0.0156,  ..., -0.0212,  0.0207,  0.0159],\n",
      "        [-0.0117, -0.0050,  0.0167,  ..., -0.0043,  0.0148,  0.0186],\n",
      "        ...,\n",
      "        [-0.0055, -0.0018,  0.0125,  ...,  0.0064,  0.0084, -0.0144],\n",
      "        [ 0.0208,  0.0058,  0.0021,  ..., -0.0150,  0.0107,  0.0129],\n",
      "        [-0.0113, -0.0089, -0.0142,  ...,  0.0176,  0.0201,  0.0203]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0171, -0.0194, -0.0197,  ...,  0.0200,  0.0033,  0.0088],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0017, -0.0027,  0.0207,  ..., -0.0093,  0.0123,  0.0033],\n",
      "        [ 0.0090,  0.0082,  0.0043,  ..., -0.0026, -0.0088, -0.0125],\n",
      "        [-0.0066,  0.0019, -0.0057,  ..., -0.0218, -0.0091,  0.0111],\n",
      "        ...,\n",
      "        [ 0.0128,  0.0107,  0.0046,  ...,  0.0140,  0.0125,  0.0086],\n",
      "        [ 0.0042, -0.0183, -0.0183,  ...,  0.0082,  0.0197,  0.0010],\n",
      "        [-0.0190,  0.0178, -0.0087,  ..., -0.0104,  0.0216,  0.0072]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0169, -0.0057, -0.0177,  ...,  0.0180, -0.0068, -0.0072],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0160,  0.0027, -0.0162,  ...,  0.0153,  0.0190, -0.0295],\n",
      "        [ 0.0204,  0.0050, -0.0152,  ...,  0.0306,  0.0187, -0.0062],\n",
      "        [-0.0140,  0.0126, -0.0048,  ...,  0.0011, -0.0056, -0.0230],\n",
      "        ...,\n",
      "        [-0.0152,  0.0202, -0.0215,  ..., -0.0120, -0.0206, -0.0204],\n",
      "        [-0.0027,  0.0058,  0.0222,  ..., -0.0310, -0.0310, -0.0130],\n",
      "        [ 0.0123, -0.0035, -0.0060,  ..., -0.0011,  0.0139, -0.0222]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0164, -0.0006,  0.0183,  0.0172, -0.0254, -0.0128, -0.0248,  0.0188,\n",
      "        -0.0229, -0.0223,  0.0308, -0.0165,  0.0093, -0.0099, -0.0195,  0.0128,\n",
      "        -0.0180, -0.0020, -0.0140, -0.0209,  0.0085, -0.0100,  0.0086, -0.0035,\n",
      "        -0.0002, -0.0307,  0.0243, -0.0158, -0.0216,  0.0207, -0.0234,  0.0246,\n",
      "         0.0031,  0.0038,  0.0055, -0.0100, -0.0113,  0.0271,  0.0075, -0.0053,\n",
      "         0.0100, -0.0136,  0.0165,  0.0292, -0.0242,  0.0311, -0.0074,  0.0132,\n",
      "         0.0169,  0.0202,  0.0096,  0.0200,  0.0159,  0.0098,  0.0088, -0.0309,\n",
      "        -0.0126,  0.0040, -0.0037,  0.0156, -0.0110, -0.0106, -0.0113,  0.0254,\n",
      "        -0.0132,  0.0275,  0.0111,  0.0039,  0.0034, -0.0027,  0.0128, -0.0042,\n",
      "         0.0172,  0.0120,  0.0182,  0.0225, -0.0116, -0.0279, -0.0055, -0.0127,\n",
      "         0.0237, -0.0168, -0.0283, -0.0282, -0.0106, -0.0150,  0.0209, -0.0135,\n",
      "        -0.0191,  0.0247, -0.0018,  0.0199,  0.0209, -0.0008, -0.0203,  0.0036,\n",
      "        -0.0167,  0.0050,  0.0145,  0.0121, -0.0312,  0.0017,  0.0093, -0.0271,\n",
      "         0.0235, -0.0306, -0.0235,  0.0196, -0.0260,  0.0156, -0.0067,  0.0043,\n",
      "        -0.0203, -0.0166, -0.0156,  0.0124,  0.0155,  0.0252,  0.0068, -0.0149,\n",
      "        -0.0022,  0.0173,  0.0077,  0.0211, -0.0037, -0.0071,  0.0185, -0.0063],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0572,  0.0083, -0.0033, -0.0513, -0.0413, -0.0286, -0.0495, -0.0259,\n",
      "          0.0555, -0.0769,  0.0527,  0.0356,  0.0730,  0.0781, -0.0479,  0.0431,\n",
      "          0.0141,  0.0128,  0.0682, -0.0691,  0.0482, -0.0735,  0.0489, -0.0798,\n",
      "          0.0619, -0.0662, -0.0371,  0.0506, -0.0253, -0.0313, -0.0176,  0.0251,\n",
      "         -0.0246, -0.0231, -0.0633, -0.0103, -0.0735, -0.0561,  0.0648,  0.0747,\n",
      "          0.0026,  0.0275,  0.0642, -0.0087, -0.0113, -0.0390,  0.0222, -0.0792,\n",
      "          0.0121, -0.0057, -0.0757, -0.0529,  0.0064,  0.0156, -0.0200,  0.0526,\n",
      "         -0.0840,  0.0077,  0.0530, -0.0587, -0.0710, -0.0539, -0.0526, -0.0843,\n",
      "         -0.0778, -0.0522,  0.0575,  0.0852,  0.0085,  0.0498,  0.0342, -0.0570,\n",
      "         -0.0815, -0.0506, -0.0033,  0.0181,  0.0636, -0.0422,  0.0735, -0.0261,\n",
      "         -0.0077, -0.0481,  0.0308,  0.0408,  0.0256, -0.0180,  0.0400,  0.0229,\n",
      "         -0.0542, -0.0259, -0.0019, -0.0078, -0.0287,  0.0633, -0.0117,  0.0297,\n",
      "         -0.0788,  0.0475, -0.0739,  0.0364,  0.0165, -0.0867, -0.0790, -0.0342,\n",
      "          0.0384, -0.0526, -0.0856,  0.0186,  0.0452,  0.0612, -0.0709, -0.0740,\n",
      "          0.0547, -0.0883, -0.0828, -0.0521, -0.0747,  0.0402, -0.0701, -0.0455,\n",
      "          0.0621, -0.0648,  0.0883,  0.0878, -0.0579,  0.0015, -0.0404, -0.0095],\n",
      "        [-0.0314,  0.0116,  0.0534, -0.0577,  0.0503, -0.0850, -0.0642, -0.0472,\n",
      "         -0.0805,  0.0762, -0.0271, -0.0158, -0.0001,  0.0102, -0.0494, -0.0109,\n",
      "          0.0007, -0.0055,  0.0495, -0.0202, -0.0265, -0.0819, -0.0761, -0.0064,\n",
      "         -0.0405, -0.0850,  0.0388,  0.0250,  0.0756, -0.0187, -0.0201, -0.0675,\n",
      "         -0.0091, -0.0426,  0.0218,  0.0494, -0.0718,  0.0702, -0.0317, -0.0668,\n",
      "          0.0021,  0.0580,  0.0039,  0.0103, -0.0867, -0.0728, -0.0424, -0.0823,\n",
      "          0.0828, -0.0570,  0.0490,  0.0726,  0.0728, -0.0619,  0.0813,  0.0323,\n",
      "          0.0680,  0.0491, -0.0632, -0.0698, -0.0103, -0.0268, -0.0142,  0.0732,\n",
      "          0.0130, -0.0103, -0.0337,  0.0442, -0.0344, -0.0856,  0.0830, -0.0336,\n",
      "          0.0816, -0.0666, -0.0799, -0.0659,  0.0119, -0.0481, -0.0228,  0.0338,\n",
      "          0.0774, -0.0449, -0.0334, -0.0331, -0.0612, -0.0688, -0.0422, -0.0284,\n",
      "         -0.0807,  0.0852, -0.0554,  0.0582, -0.0336, -0.0278,  0.0720,  0.0655,\n",
      "         -0.0863,  0.0719,  0.0452,  0.0066, -0.0705,  0.0009,  0.0416, -0.0412,\n",
      "         -0.0810,  0.0008, -0.0370, -0.0502,  0.0315,  0.0334,  0.0118,  0.0880,\n",
      "          0.0577,  0.0226, -0.0447, -0.0863, -0.0603,  0.0107, -0.0336,  0.0334,\n",
      "          0.0345, -0.0596,  0.0582,  0.0488,  0.0688,  0.0766,  0.0200,  0.0016],\n",
      "        [-0.0315, -0.0623, -0.0470, -0.0496, -0.0163, -0.0812, -0.0613, -0.0868,\n",
      "          0.0559, -0.0494, -0.0440,  0.0010, -0.0860,  0.0763, -0.0464,  0.0445,\n",
      "          0.0152,  0.0121,  0.0066, -0.0010,  0.0571, -0.0306,  0.0383, -0.0467,\n",
      "          0.0180,  0.0158,  0.0785, -0.0861,  0.0490,  0.0426,  0.0720, -0.0140,\n",
      "          0.0081, -0.0275, -0.0527, -0.0425,  0.0242, -0.0711,  0.0298,  0.0240,\n",
      "          0.0843,  0.0629,  0.0738,  0.0450,  0.0825,  0.0531, -0.0088, -0.0250,\n",
      "         -0.0137, -0.0491,  0.0461,  0.0015,  0.0241,  0.0185,  0.0036,  0.0386,\n",
      "         -0.0853, -0.0816, -0.0505,  0.0243,  0.0047, -0.0239, -0.0379,  0.0040,\n",
      "         -0.0241, -0.0139, -0.0058,  0.0364,  0.0040, -0.0495, -0.0066,  0.0277,\n",
      "          0.0306, -0.0788, -0.0497,  0.0493,  0.0564, -0.0127, -0.0069, -0.0185,\n",
      "          0.0684, -0.0661,  0.0104, -0.0295, -0.0163,  0.0536, -0.0115,  0.0597,\n",
      "         -0.0541,  0.0266,  0.0341, -0.0039,  0.0377,  0.0177,  0.0771, -0.0789,\n",
      "          0.0405, -0.0502,  0.0347,  0.0416, -0.0038,  0.0121, -0.0181,  0.0212,\n",
      "         -0.0407, -0.0299,  0.0036, -0.0115, -0.0483,  0.0313,  0.0341, -0.0163,\n",
      "          0.0149,  0.0528,  0.0408,  0.0614,  0.0320,  0.0608, -0.0302,  0.0385,\n",
      "         -0.0636,  0.0158,  0.0127,  0.0510, -0.0792, -0.0296, -0.0442, -0.0266],\n",
      "        [ 0.0849,  0.0820,  0.0704, -0.0799, -0.0426,  0.0684,  0.0106,  0.0321,\n",
      "          0.0460,  0.0792,  0.0723, -0.0308,  0.0618, -0.0571, -0.0009, -0.0848,\n",
      "         -0.0136, -0.0125,  0.0705, -0.0231, -0.0672, -0.0131, -0.0340, -0.0570,\n",
      "         -0.0457, -0.0308,  0.0090,  0.0204,  0.0484,  0.0316,  0.0626, -0.0063,\n",
      "          0.0544,  0.0584,  0.0762,  0.0326, -0.0113,  0.0802, -0.0522, -0.0092,\n",
      "          0.0588, -0.0400,  0.0652,  0.0480, -0.0460, -0.0306,  0.0090, -0.0437,\n",
      "         -0.0691,  0.0235,  0.0279,  0.0434, -0.0559,  0.0321, -0.0556, -0.0794,\n",
      "         -0.0419, -0.0338,  0.0237, -0.0574,  0.0271, -0.0452, -0.0560,  0.0447,\n",
      "          0.0712,  0.0811,  0.0623, -0.0009, -0.0473, -0.0563,  0.0069,  0.0114,\n",
      "         -0.0669,  0.0269,  0.0712, -0.0591, -0.0301, -0.0693, -0.0468, -0.0292,\n",
      "          0.0299, -0.0034,  0.0100,  0.0768,  0.0596,  0.0590, -0.0185,  0.0161,\n",
      "         -0.0240, -0.0386,  0.0164, -0.0692,  0.0742, -0.0609, -0.0219,  0.0058,\n",
      "          0.0465,  0.0328, -0.0419, -0.0425,  0.0697,  0.0464, -0.0306,  0.0263,\n",
      "         -0.0400, -0.0744, -0.0659, -0.0170,  0.0380,  0.0030,  0.0722,  0.0395,\n",
      "          0.0761, -0.0884, -0.0043, -0.0256,  0.0531, -0.0390, -0.0163, -0.0017,\n",
      "          0.0546, -0.0746, -0.0155, -0.0023, -0.0363,  0.0605, -0.0853, -0.0386]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0260,  0.0438, -0.0016,  0.0611], requires_grad=True)\n",
      "2nd layer params\n",
      "Parameter containing:\n",
      "tensor([[ 0.0017, -0.0027,  0.0207,  ..., -0.0093,  0.0123,  0.0033],\n",
      "        [ 0.0090,  0.0082,  0.0043,  ..., -0.0026, -0.0088, -0.0125],\n",
      "        [-0.0066,  0.0019, -0.0057,  ..., -0.0218, -0.0091,  0.0111],\n",
      "        ...,\n",
      "        [ 0.0128,  0.0107,  0.0046,  ...,  0.0140,  0.0125,  0.0086],\n",
      "        [ 0.0042, -0.0183, -0.0183,  ...,  0.0082,  0.0197,  0.0010],\n",
      "        [-0.0190,  0.0178, -0.0087,  ..., -0.0104,  0.0216,  0.0072]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0169, -0.0057, -0.0177,  ...,  0.0180, -0.0068, -0.0072],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "title_model = TitlePartyModel()\n",
    "print(\"the model\")\n",
    "print(title_model)\n",
    "print(\"just the 2nd layer\")\n",
    "print(title_model.hidden1)\n",
    "print(\"parameters\")\n",
    "for p in title_model.parameters():\n",
    "    print(p)\n",
    "print(\"2nd layer params\")\n",
    "for p in title_model.hidden1.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import fnmatch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(title_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\n",
      "train: summary_bill_1811_1393180-shrunk.pkl\n",
      "train: summary_bill_1811_1393181-shrunk.pkl\n",
      "train: summary_bill_1811_1470063-shrunk.pkl\n",
      "test : summary_bill_1811_1506887-shrunk.pkl\n",
      "train: summary_bill_1959_1542899-shrunk.pkl\n",
      "train: summary_bill_1959_1545862-shrunk.pkl\n",
      "train: summary_bill_1959_1546074-shrunk.pkl\n",
      "train: summary_bill_1959_1546096-shrunk.pkl\n",
      "['C:\\\\Users\\\\benja\\\\git-projects\\\\bitbucket\\\\nlp_legislation_prediction\\\\training\\\\..\\\\data\\\\tokenized\\\\summary_bill_1811_1393180-shrunk.pkl', 'C:\\\\Users\\\\benja\\\\git-projects\\\\bitbucket\\\\nlp_legislation_prediction\\\\training\\\\..\\\\data\\\\tokenized\\\\summary_bill_1811_1393181-shrunk.pkl', 'C:\\\\Users\\\\benja\\\\git-projects\\\\bitbucket\\\\nlp_legislation_prediction\\\\training\\\\..\\\\data\\\\tokenized\\\\summary_bill_1811_1470063-shrunk.pkl', 'C:\\\\Users\\\\benja\\\\git-projects\\\\bitbucket\\\\nlp_legislation_prediction\\\\training\\\\..\\\\data\\\\tokenized\\\\summary_bill_1959_1542899-shrunk.pkl', 'C:\\\\Users\\\\benja\\\\git-projects\\\\bitbucket\\\\nlp_legislation_prediction\\\\training\\\\..\\\\data\\\\tokenized\\\\summary_bill_1959_1545862-shrunk.pkl', 'C:\\\\Users\\\\benja\\\\git-projects\\\\bitbucket\\\\nlp_legislation_prediction\\\\training\\\\..\\\\data\\\\tokenized\\\\summary_bill_1959_1546074-shrunk.pkl', 'C:\\\\Users\\\\benja\\\\git-projects\\\\bitbucket\\\\nlp_legislation_prediction\\\\training\\\\..\\\\data\\\\tokenized\\\\summary_bill_1959_1546096-shrunk.pkl']\n",
      "['C:\\\\Users\\\\benja\\\\git-projects\\\\bitbucket\\\\nlp_legislation_prediction\\\\training\\\\..\\\\data\\\\tokenized\\\\summary_bill_1811_1506887-shrunk.pkl']\n"
     ]
    }
   ],
   "source": [
    "train_files = []\n",
    "test_files = []\n",
    "split = 0.7\n",
    "token_path = os.path.join(os.getcwd(),\"..\",\"data\",\"tokenized\")\n",
    "print(token_path)\n",
    "for root, dirs, files in os.walk(token_path):\n",
    "    for f in files:\n",
    "        if fnmatch.fnmatch(f, \"*shrunk*\"):\n",
    "            if np.random.sample(1) <= split:\n",
    "                print(f'train: {f}')\n",
    "                train_files.append(os.path.join(root, f))\n",
    "            else:\n",
    "                print(f'test : {f}')\n",
    "                test_files.append(os.path.join(root, f))\n",
    "print(train_files)\n",
    "print(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SummaryDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path_arr):\n",
    "        self.data_frames = []\n",
    "        for f in file_path_arr:\n",
    "            print(f\"loading {f}\")\n",
    "            self.data_frames.append(pd.read_pickle(f, compression=\"gzip\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "#        return len(self.data_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        next_df = self.data_frames[0]\n",
    "        party = next_df[\"party\"][0] # they are all the same, so just pick the first one\n",
    "        encoding = torch.tensor(np.array(next_df[\"input_shrunk\"]),dtype=torch.float)\n",
    "        # 4 politcal party choices\n",
    "        party_arr = np.zeros(4,dtype=int)\n",
    "        # the party index was stored as value with a starting index of 1 -- rethink this\n",
    "        party_arr[party-1] = 1 # set the value to 1 for the party index\n",
    "        return encoding, torch.tensor(party_arr,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\\summary_bill_1811_1393180-shrunk.pkl\n",
      "loading C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\\summary_bill_1811_1393181-shrunk.pkl\n",
      "loading C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\\summary_bill_1811_1470063-shrunk.pkl\n",
      "loading C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\\summary_bill_1959_1542899-shrunk.pkl\n",
      "loading C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\\summary_bill_1959_1545862-shrunk.pkl\n",
      "loading C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\\summary_bill_1959_1546074-shrunk.pkl\n",
      "loading C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\\summary_bill_1959_1546096-shrunk.pkl\n",
      "loading C:\\Users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\training\\..\\data\\tokenized\\summary_bill_1811_1506887-shrunk.pkl\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_data_set = SummaryDataSet(train_files)\n",
    "test_data_set = SummaryDataSet(test_files)\n",
    "train_loader = DataLoader(train_data_set,batch_size=1,shuffle=True)\n",
    "test_loader = DataLoader(test_data_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_idx, model, summary_writer):\n",
    "    running_loss = 0\n",
    "    last_loss = 0.\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, label = data\n",
    "        input_tensor = inputs.view(1,-1)\n",
    "        outputs = model(input_tensor)\n",
    "        loss = loss_fn(outputs, label)\n",
    "        print(type(loss))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(type(running_loss))\n",
    "        last_loss = running_loss\n",
    "        summary_idx = i * len(train_loader) + i + 1\n",
    "        summary_writer.add_scalar(\"loss/train\", last_loss, summary_idx)\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "title_model = TitlePartyModel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_37412\\2359741441.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.output(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n",
      "turn on training\n",
      "running one epoch\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "turn off training\n",
      "epoch loss 0.1541554182767868\n",
      "label : tensor([[0., 0., 0., 1.]])\n",
      "vOutput: tensor([[0.3229, 0.2860, 0.2060, 0.1851]])\n",
      "LOSS train 0.1541554182767868 valid 0.22315868735313416\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(start_time))\n",
    "EPOCHS = 10\n",
    "epoch_num = 0\n",
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"turn on training\")\n",
    "    title_model.train(True)\n",
    "    print(\"running one epoch\")\n",
    "    last_epoch_loss = train_one_epoch(epoch_num,title_model, writer)\n",
    "    print(\"turn off training\")\n",
    "    print(f'epoch loss {last_epoch_loss}')\n",
    "\n",
    "    # title_model.train(False)\n",
    "\n",
    "    running_validation_loss = 0.0\n",
    "    with (torch.no_grad()):\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            vinputs, vlabel = vdata\n",
    "            print(f\"label : {vlabel}\")\n",
    "            voutputs = title_model(vinputs)\n",
    "            print(f\"vOutput: {voutputs}\")\n",
    "            vloss = loss_fn(voutputs, vlabel)\n",
    "            running_validation_loss += vloss\n",
    "\n",
    "    avg_vloss = running_validation_loss / len(test_loader)\n",
    "    losses.append(avg_vloss)\n",
    "    print('LOSS train {} valid {}'.format(last_epoch_loss, avg_vloss))\n",
    "#    writer.add_scalars(\"Training vs Valiation loss\",{\"training\": last_epoch_loss, \"validation\": avg_vloss}, epoch_num+1)\n",
    "#    writer.flush()\n",
    "    epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>), tensor(1.4816, grad_fn=<DivBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}