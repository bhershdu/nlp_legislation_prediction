{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# An example of using the Bert model with pytorch\n",
    "https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "unmasker = pipeline(\"fill-mask\", model='bert-base-cased')\n",
    "unmasker(\"Data Science is a [MASK] topic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded input\n",
      "{'input_ids': tensor([[  101,  3396, 18417,  1105,  5334,  5848,  2403,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "model output\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0954,  0.0860, -0.1755,  ..., -0.1215,  0.2177, -0.1546],\n",
      "         [-0.2367, -0.5680, -0.0705,  ...,  0.2298,  0.5074, -0.3337],\n",
      "         [ 0.1411, -0.6689,  0.3907,  ..., -0.2502,  0.5436, -0.1273],\n",
      "         ...,\n",
      "         [ 0.2350,  0.1372,  0.0779,  ...,  0.5715, -0.0502, -0.6023],\n",
      "         [-0.0924, -0.8698, -0.2965,  ..., -0.4819,  0.1679, -0.3317],\n",
      "         [ 0.1322, -0.5264, -0.5078,  ...,  0.0271, -0.0999, -0.4042]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-5.4351e-01,  4.3489e-01,  9.9989e-01, -9.9644e-01,  9.6890e-01,\n",
      "          9.1887e-01,  9.8558e-01, -9.8568e-01, -9.9278e-01, -5.3747e-01,\n",
      "          9.9226e-01,  9.9943e-01, -9.9700e-01, -9.9982e-01,  8.1915e-01,\n",
      "         -9.8704e-01,  9.9248e-01, -4.9037e-01, -9.9999e-01, -7.4525e-01,\n",
      "         -3.7347e-01, -9.9994e-01,  3.6646e-01,  9.7447e-01,  9.7824e-01,\n",
      "          1.2637e-01,  9.9354e-01,  9.9998e-01,  7.9105e-01,  2.7707e-01,\n",
      "          2.8634e-01, -9.9326e-01,  7.5334e-01, -9.9921e-01,  1.6726e-01,\n",
      "          1.8330e-01,  5.7417e-01, -2.2537e-01,  7.7339e-01, -9.4105e-01,\n",
      "         -6.0673e-01, -3.5201e-01,  6.1615e-01, -5.9545e-01,  9.5316e-01,\n",
      "          1.8367e-01,  2.0717e-01, -7.5895e-02, -1.1531e-01,  9.9977e-01,\n",
      "         -9.5999e-01,  9.9954e-01, -9.7945e-01,  9.9758e-01,  9.9854e-01,\n",
      "          3.1009e-01,  9.9808e-01,  1.5914e-01, -9.9673e-01,  1.6850e-01,\n",
      "          9.7363e-01,  1.3842e-01,  9.2063e-01, -3.7061e-01,  1.0653e-02,\n",
      "         -3.8447e-01, -8.6480e-01,  2.4282e-01, -6.1070e-01,  3.0708e-01,\n",
      "          3.3809e-01,  3.6943e-01,  9.9261e-01, -9.4007e-01,  5.9783e-02,\n",
      "         -9.2253e-01,  3.7753e-01, -9.9995e-01,  9.4522e-01,  9.9998e-01,\n",
      "          6.6246e-01, -9.9989e-01,  9.9759e-01, -2.0205e-01, -6.6016e-01,\n",
      "          4.0051e-01, -9.9772e-01, -9.9980e-01,  1.0672e-01, -6.5137e-01,\n",
      "          8.7152e-01, -9.9356e-01,  1.2273e-01, -8.9588e-01,  9.9999e-01,\n",
      "         -8.1513e-01, -1.2461e-01,  4.3518e-01,  9.3007e-01, -6.7836e-01,\n",
      "         -7.0764e-01,  8.8545e-01,  9.9787e-01, -9.9070e-01,  9.9683e-01,\n",
      "          5.1725e-01, -9.3279e-01, -8.2487e-01,  6.8430e-01,  5.8711e-02,\n",
      "          9.9030e-01, -9.9034e-01, -7.0906e-01,  1.9693e-01,  9.3472e-01,\n",
      "         -7.7246e-01,  9.9674e-01,  6.6154e-01, -1.9586e-01,  9.9999e-01,\n",
      "         -1.0889e-01,  9.7302e-01,  9.9931e-01,  8.5941e-01, -6.9713e-01,\n",
      "         -1.7905e-01, -7.7775e-02,  8.5954e-01, -4.7608e-01, -4.3934e-01,\n",
      "          8.3116e-01, -9.9628e-01, -9.9684e-01,  9.9977e-01, -2.9810e-01,\n",
      "          9.9999e-01, -9.9967e-01,  9.8873e-01, -9.9997e-01, -7.4408e-01,\n",
      "         -8.6329e-01,  3.0261e-02, -9.8614e-01,  6.4913e-01,  9.9718e-01,\n",
      "          6.4892e-02, -9.2876e-01, -8.4177e-01,  7.7315e-01, -8.1832e-01,\n",
      "          4.6787e-01,  7.7898e-01, -9.5027e-01,  9.9813e-01,  9.9498e-01,\n",
      "          9.5251e-01,  9.8970e-01,  1.8339e-01, -9.5833e-01,  8.4807e-01,\n",
      "          9.9484e-01, -9.9979e-01,  9.2133e-01, -9.9050e-01,  9.9945e-01,\n",
      "          9.8210e-01,  7.3574e-01, -9.9136e-01,  9.9995e-01, -6.0010e-01,\n",
      "          9.5380e-02, -6.1743e-01, -2.7125e-01, -9.9822e-01,  4.8218e-01,\n",
      "          5.0433e-01,  7.0617e-01,  9.9943e-01, -9.9705e-01,  9.9959e-01,\n",
      "          9.7105e-01, -1.5655e-02,  7.5713e-01,  9.9690e-01, -9.9809e-01,\n",
      "         -9.8688e-01, -9.9457e-01,  4.5816e-01,  8.0075e-01,  6.9611e-01,\n",
      "          4.1278e-01,  9.6786e-01,  9.9785e-01,  6.0412e-01, -9.9631e-01,\n",
      "         -4.9113e-01,  9.8796e-01, -2.9093e-01,  9.9999e-01, -2.1659e-01,\n",
      "         -9.9990e-01, -7.2700e-01,  9.3094e-01,  9.9678e-01, -3.4237e-01,\n",
      "          9.9098e-01, -4.7236e-01, -1.1919e-01,  9.6505e-01, -9.9753e-01,\n",
      "          9.9638e-01,  7.6153e-02,  8.3854e-01,  8.3496e-01,  9.9678e-01,\n",
      "         -6.7651e-01, -1.5190e-01,  1.4926e-01, -5.8083e-01,  9.9996e-01,\n",
      "         -9.9982e-01, -3.8026e-01,  4.6360e-01, -9.9648e-01, -9.9943e-01,\n",
      "          9.8140e-01, -6.2111e-02, -6.8390e-01, -3.6973e-01,  3.0405e-01,\n",
      "          2.1285e-01,  9.0141e-01,  9.9596e-01, -3.7548e-01, -5.4111e-01,\n",
      "         -9.9989e-01, -9.9565e-01, -8.1109e-01, -9.3612e-01,  1.7681e-01,\n",
      "          6.5505e-01, -3.6840e-01, -9.6764e-01, -9.9733e-01,  9.6798e-01,\n",
      "          6.7289e-02, -9.0441e-01,  3.9795e-01, -2.3494e-01, -9.9724e-01,\n",
      "          5.3921e-01, -8.1030e-01, -9.9908e-01,  9.9976e-01, -6.4156e-01,\n",
      "          9.9152e-01,  9.8663e-01, -9.9697e-01,  5.8201e-01, -9.9784e-01,\n",
      "         -3.6500e-04, -9.9869e-01,  4.2268e-01,  5.8029e-01, -5.6792e-01,\n",
      "         -4.8874e-02,  9.9715e-01, -9.6368e-01, -6.7664e-01,  8.4041e-01,\n",
      "         -9.9993e-01,  9.5539e-01, -3.4068e-01,  9.9957e-01,  8.5300e-01,\n",
      "          3.4266e-01,  9.9362e-01,  9.3567e-01, -9.9382e-01, -9.9991e-01,\n",
      "          8.9649e-01,  9.8740e-01, -9.9568e-01, -3.0775e-01,  9.9997e-01,\n",
      "         -9.9652e-01, -8.6715e-01, -9.1422e-01, -9.9807e-01, -9.9983e-01,\n",
      "          6.2626e-02, -6.3991e-01,  6.1124e-02,  9.8352e-01, -1.4633e-02,\n",
      "          1.4074e-01,  9.9775e-01,  9.9756e-01,  1.6621e-01, -1.4183e-01,\n",
      "         -1.8640e-02, -9.6017e-01, -9.8921e-01,  2.7269e-01,  1.8513e-01,\n",
      "         -9.9998e-01,  9.9992e-01, -9.9861e-01,  9.9900e-01,  9.6601e-01,\n",
      "         -9.8471e-01,  7.8447e-01,  6.7431e-02, -9.6004e-01,  8.7747e-02,\n",
      "          9.9997e-01,  9.8558e-01, -5.3247e-02,  2.6416e-01,  9.0714e-01,\n",
      "         -2.8704e-01,  5.1184e-01, -8.4844e-01, -5.0166e-01,  2.0024e-01,\n",
      "         -9.4400e-01,  9.9162e-01,  6.6521e-01, -9.9584e-01,  9.9319e-01,\n",
      "          1.5201e-01,  7.5451e-01, -6.5815e-01,  8.2370e-01,  9.9358e-01,\n",
      "         -1.7316e-01, -4.3644e-01, -2.1338e-01, -7.2286e-01, -9.6145e-01,\n",
      "          1.5671e-01, -9.9457e-01, -1.4850e-01,  9.3072e-01,  9.9538e-01,\n",
      "         -9.9767e-01,  9.9777e-01, -2.7545e-01,  8.4289e-01, -9.9688e-01,\n",
      "          9.9999e-01, -9.9900e-01,  1.4411e-01,  7.4368e-01, -8.8546e-01,\n",
      "         -1.5587e-01,  9.9301e-01,  9.6750e-01,  9.5185e-01, -8.8397e-01,\n",
      "         -7.9750e-01,  9.0296e-01,  9.8862e-01, -9.7563e-01,  4.6465e-02,\n",
      "         -9.9895e-01, -6.7964e-01,  9.9159e-01,  9.8864e-01, -5.3200e-02,\n",
      "         -6.7111e-01, -9.9600e-01,  9.6429e-01, -8.6317e-01, -8.0671e-01,\n",
      "         -1.0813e-01, -7.5730e-01,  4.4819e-01,  9.9586e-01, -2.6955e-01,\n",
      "          6.8280e-01,  2.0502e-01, -9.9306e-01,  6.4465e-01,  7.6394e-01,\n",
      "          9.9992e-01, -9.8926e-01,  6.2141e-01,  9.9455e-01, -2.3282e-01,\n",
      "         -5.8258e-01,  6.2221e-01,  9.9790e-01, -9.8503e-01, -3.2796e-01,\n",
      "         -9.9984e-01, -3.7136e-03, -7.2494e-01, -1.8335e-03, -2.4818e-01,\n",
      "          9.7617e-02, -7.2345e-01,  9.6846e-01, -5.9032e-02,  8.0361e-01,\n",
      "         -2.3786e-01,  9.6975e-01, -3.6123e-01, -1.3584e-01, -3.1858e-01,\n",
      "          1.7710e-01,  4.7628e-01,  2.1454e-01,  9.8800e-01, -9.7601e-01,\n",
      "          9.9983e-01, -2.7359e-01, -9.9998e-01, -9.9481e-01, -4.5936e-01,\n",
      "         -9.9965e-01,  4.6013e-01, -9.9758e-01,  9.9533e-01,  9.0952e-01,\n",
      "         -9.9747e-01, -9.9795e-01, -9.9904e-01, -9.9865e-01,  8.3002e-01,\n",
      "          4.8569e-01, -9.4246e-02,  2.6136e-01,  5.1679e-01,  8.7389e-02,\n",
      "         -4.1524e-01,  3.3748e-03, -9.5046e-01, -6.3318e-01, -9.9703e-01,\n",
      "          5.9417e-01, -9.9998e-01, -7.7892e-01,  9.9528e-01, -9.9277e-01,\n",
      "         -9.3838e-01, -9.6374e-01, -8.3105e-01, -8.7298e-01,  4.2248e-01,\n",
      "          9.9138e-01,  1.9798e-01, -7.0689e-01, -9.9949e-01,  9.9390e-01,\n",
      "         -7.6583e-01,  2.5367e-01, -8.2039e-01, -9.8532e-01,  9.9982e-01,\n",
      "          7.8163e-01, -9.4027e-02, -6.9375e-02, -9.9948e-01,  9.8831e-01,\n",
      "         -9.4561e-01, -8.7203e-01, -9.8899e-01,  2.0724e-01, -9.5388e-01,\n",
      "         -9.9992e-01,  2.1394e-01,  9.9535e-01,  9.9817e-01,  9.8430e-01,\n",
      "          2.3595e-01, -3.7110e-01, -9.6808e-01,  1.0993e-01, -9.9998e-01,\n",
      "          8.3824e-01,  7.6857e-01, -9.8990e-01, -6.8750e-01,  9.9752e-01,\n",
      "          9.8978e-01, -9.2739e-01, -9.7713e-01,  9.4710e-01,  4.5394e-01,\n",
      "          9.6307e-01, -4.8390e-01, -5.4108e-01,  3.1930e-01, -7.7465e-02,\n",
      "         -9.9457e-01, -9.0099e-01,  9.9869e-01, -9.9854e-01,  9.8614e-01,\n",
      "          9.9582e-01,  9.9752e-01, -2.7927e-02,  3.4827e-02, -9.8531e-01,\n",
      "         -9.9734e-01, -6.6752e-01,  3.8999e-01, -9.9996e-01,  9.9992e-01,\n",
      "         -9.9998e-01,  5.9683e-01, -7.1480e-01,  7.0284e-01,  9.9601e-01,\n",
      "         -8.9532e-02, -9.9996e-01, -9.9991e-01,  1.8360e-01, -1.3248e-01,\n",
      "          9.9576e-01,  1.5330e-01,  2.9289e-01, -3.2753e-01, -7.8391e-01,\n",
      "          9.9905e-01, -9.0677e-01, -7.5427e-01, -9.9458e-01,  9.9984e-01,\n",
      "          7.1164e-01, -9.9932e-01,  9.9520e-01, -9.9975e-01,  7.9623e-01,\n",
      "          9.8570e-01,  9.1134e-01,  9.6759e-01, -9.9784e-01,  9.9999e-01,\n",
      "         -9.9989e-01,  9.9790e-01, -9.9999e-01, -9.9747e-01,  9.9993e-01,\n",
      "         -9.9295e-01, -6.5569e-01, -9.9982e-01, -9.9762e-01,  5.9706e-01,\n",
      "          2.3836e-01, -6.5957e-01,  9.9609e-01, -9.9990e-01, -9.9946e-01,\n",
      "          3.2320e-01, -9.1058e-01, -9.1222e-01,  9.9271e-01, -4.4286e-01,\n",
      "          9.9703e-01, -1.2560e-01,  9.5650e-01,  1.1046e-01,  9.9361e-01,\n",
      "          9.9909e-01, -6.0145e-01, -7.5135e-01, -9.9657e-01,  9.9065e-01,\n",
      "         -7.8485e-01,  4.0804e-01,  9.5499e-01, -1.0814e-01, -5.5177e-01,\n",
      "          5.3439e-01, -9.9908e-01,  4.6566e-01, -3.6461e-01,  8.1043e-01,\n",
      "          9.5035e-01,  8.0810e-01,  1.1972e-01, -4.9862e-01, -1.8213e-01,\n",
      "         -9.9631e-01,  5.0803e-01, -9.9968e-01,  9.7313e-01, -9.2221e-01,\n",
      "          1.8008e-02, -5.0409e-01,  6.2895e-01, -9.7083e-01,  9.9966e-01,\n",
      "          9.9942e-01, -9.9936e-01,  1.3345e-01,  9.9563e-01, -4.3109e-01,\n",
      "          9.6779e-01, -9.9760e-01, -3.4068e-04,  9.4506e-01, -6.7993e-01,\n",
      "          9.9273e-01,  2.7220e-01, -1.0653e-01,  9.6836e-01, -9.9845e-01,\n",
      "         -8.8886e-01, -7.0413e-01,  2.8267e-01, -8.0461e-02, -9.8330e-01,\n",
      "          1.3966e-01,  9.8301e-01, -1.2253e-01, -9.9986e-01,  9.7421e-01,\n",
      "         -9.9971e-01, -8.1672e-02,  9.9161e-01,  5.7556e-01,  9.9997e-01,\n",
      "         -6.8349e-01,  7.8376e-02, -5.5426e-02, -9.9992e-01, -9.9840e-01,\n",
      "          1.5137e-01, -3.3087e-01, -9.3070e-01,  9.9869e-01,  5.0286e-02,\n",
      "          8.5629e-01, -9.9993e-01,  1.9415e-01,  9.9535e-01,  2.6235e-01,\n",
      "          7.6839e-01, -8.5605e-01, -9.6119e-01, -9.1121e-01, -5.1040e-01,\n",
      "          1.2157e-01,  7.8097e-01, -9.9694e-01, -8.8622e-01, -7.7598e-01,\n",
      "          9.9998e-01, -9.9928e-01, -8.3523e-01, -9.8591e-01,  5.9470e-01,\n",
      "          8.5349e-01,  4.3055e-01,  7.8627e-02, -7.9719e-01,  9.1995e-01,\n",
      "         -8.4813e-01,  9.9864e-01, -9.9813e-01, -9.9831e-01,  9.9992e-01,\n",
      "          2.5337e-01, -9.8736e-01, -5.1471e-02, -4.6612e-01,  1.2542e-01,\n",
      "          6.2047e-02,  4.1926e-01, -9.4809e-01, -1.3313e-01, -9.9921e-01,\n",
      "          7.2417e-01, -7.1446e-01, -9.9714e-01, -5.8924e-01, -5.3770e-01,\n",
      "         -9.9906e-01,  9.9797e-01,  9.8411e-01,  9.9998e-01, -9.9991e-01,\n",
      "          8.2896e-01,  1.2628e-01,  9.9965e-01,  3.8223e-02, -7.4400e-01,\n",
      "          9.1421e-01,  9.9984e-01, -6.7277e-01,  7.3975e-01, -5.6030e-03,\n",
      "         -2.1580e-01,  3.7989e-01, -6.9125e-01,  9.9359e-01, -9.4656e-01,\n",
      "          4.5605e-01, -9.9136e-01, -9.9995e-01,  9.9994e-01,  1.2323e-02,\n",
      "          9.9463e-01,  1.9989e-01,  7.9085e-01, -8.8784e-01,  9.7734e-01,\n",
      "         -9.8334e-01, -8.8300e-01, -9.9998e-01,  2.8220e-01, -9.9940e-01,\n",
      "         -9.9454e-01,  1.8941e-01,  9.9514e-01, -9.9979e-01, -9.9619e-01,\n",
      "         -4.0144e-01, -9.9999e-01,  8.5098e-01, -9.9210e-01, -8.2116e-01,\n",
      "         -9.9433e-01,  9.9668e-01, -6.8748e-02, -5.4010e-01,  9.8067e-01,\n",
      "         -9.8364e-01,  9.3781e-01,  9.4183e-01,  2.8204e-01,  2.2903e-01,\n",
      "          3.8541e-01, -6.7195e-01, -9.9488e-01, -9.5444e-01, -9.6736e-01,\n",
      "          9.3946e-01, -9.9610e-01, -8.6811e-01,  9.9827e-01,  9.9599e-01,\n",
      "         -9.9933e-01, -9.9819e-01,  9.9237e-01, -4.3249e-01,  9.9497e-01,\n",
      "         -5.1002e-01, -9.9994e-01, -9.9996e-01,  2.0407e-01, -1.3536e-01,\n",
      "          9.9807e-01, -3.5872e-01,  9.9589e-01,  6.3056e-01,  1.1701e-01,\n",
      "          3.6037e-01, -5.9493e-01, -1.8108e-01, -6.9246e-01, -2.2518e-01,\n",
      "          9.9998e-01, -5.8045e-01,  9.9697e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "text = \"Four Score and Seven Years ago\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(\"encoded input\")\n",
    "print(encoded_input)\n",
    "print(\"model output\")\n",
    "output = model(**encoded_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0954,  0.0860, -0.1755,  ..., -0.1215,  0.2177, -0.1546],\n         [-0.2367, -0.5680, -0.0705,  ...,  0.2298,  0.5074, -0.3337],\n         [ 0.1411, -0.6689,  0.3907,  ..., -0.2502,  0.5436, -0.1273],\n         ...,\n         [ 0.2350,  0.1372,  0.0779,  ...,  0.5715, -0.0502, -0.6023],\n         [-0.0924, -0.8698, -0.2965,  ..., -0.4819,  0.1679, -0.3317],\n         [ 0.1322, -0.5264, -0.5078,  ...,  0.0271, -0.0999, -0.4042]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6144\n",
      "(6144,)\n"
     ]
    }
   ],
   "source": [
    "last_shape = output.last_hidden_state.shape\n",
    "elements = np.cumproduct(last_shape)\n",
    "print(np.max(elements))\n",
    "last_layer_vector = output.last_hidden_state.detach().numpy().reshape(max(elements))\n",
    "print(last_layer_vector.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "np.save(\"layer.npy\", last_layer_vector,allow_pickle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.09544564,  0.08602602, -0.17547004, ...,  0.02706809,\n       -0.09994922, -0.4041806 ], dtype=float32)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"layer.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('embeddings',\n              BertEmbeddings(\n                (word_embeddings): Embedding(28996, 768, padding_idx=0)\n                (position_embeddings): Embedding(512, 768)\n                (token_type_embeddings): Embedding(2, 768)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )),\n             ('encoder',\n              BertEncoder(\n                (layer): ModuleList(\n                  (0): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (1): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (2): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (3): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (4): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (5): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (6): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (7): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (8): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (9): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (10): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                  (11): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=768, bias=True)\n                        (key): Linear(in_features=768, out_features=768, bias=True)\n                        (value): Linear(in_features=768, out_features=768, bias=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (output): BertSelfOutput(\n                        (dense): Linear(in_features=768, out_features=768, bias=True)\n                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                    )\n                    (intermediate): BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n                      (intermediate_act_fn): GELUActivation()\n                    )\n                    (output): BertOutput(\n                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                  )\n                )\n              )),\n             ('pooler',\n              BertPooler(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (activation): Tanh()\n              ))])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "return_nodes = {\"encoder\":\"encoder_layer\"}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You cannot specify both input_ids and inputs_embeds at the same time",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model2 \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_feature_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_nodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoder\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\venvs3\\lib\\site-packages\\torchvision\\models\\feature_extraction.py:441\u001B[0m, in \u001B[0;36mcreate_feature_extractor\u001B[1;34m(model, return_nodes, train_return_nodes, eval_return_nodes, tracer_kwargs, suppress_diff_warning)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;66;03m# Instantiate our NodePathTracer and use that to trace the model\u001B[39;00m\n\u001B[0;32m    440\u001B[0m tracer \u001B[38;5;241m=\u001B[39m NodePathTracer(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtracer_kwargs)\n\u001B[1;32m--> 441\u001B[0m graph \u001B[38;5;241m=\u001B[39m \u001B[43mtracer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrace\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    443\u001B[0m name \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[0;32m    444\u001B[0m     model, nn\u001B[38;5;241m.\u001B[39mModule) \u001B[38;5;28;01melse\u001B[39;00m model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m    445\u001B[0m graph_module \u001B[38;5;241m=\u001B[39m fx\u001B[38;5;241m.\u001B[39mGraphModule(tracer\u001B[38;5;241m.\u001B[39mroot, graph, name)\n",
      "File \u001B[1;32mc:\\users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\venvs3\\lib\\site-packages\\torch\\fx\\_symbolic_trace.py:615\u001B[0m, in \u001B[0;36mTracer.trace\u001B[1;34m(self, root, concrete_args)\u001B[0m\n\u001B[0;32m    613\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_autowrap_search:\n\u001B[0;32m    614\u001B[0m             _autowrap_check(patcher, module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_autowrap_function_ids)\n\u001B[1;32m--> 615\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_node(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_arg(\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m),), {},\n\u001B[0;32m    616\u001B[0m                          type_expr\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__annotations__\u001B[39m\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubmodule_paths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    620\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph\n",
      "File \u001B[1;32mc:\\users\\benja\\git-projects\\bitbucket\\nlp_legislation_prediction\\venvs3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:964\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    961\u001B[0m     use_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    965\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    966\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39msize()\n",
      "\u001B[1;31mValueError\u001B[0m: You cannot specify both input_ids and inputs_embeds at the same time"
     ]
    }
   ],
   "source": [
    "model2 = create_feature_extractor(model, return_nodes=[\"encoder\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}